---
title: "Practical Machine Learning: Barbell Lifts"
author: "Hector Mario Romer"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {
        text-align: justify
}
</style>

## Overview
The goal of this project is to predict the manner in which a group of enthusiasts perform barbell lifts. We need to create a report describing how the model was built, how cross validation was used, what is the expected sample error and why choices were made during the analysis. The resulting model will be used to predict 20 different test cases.

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

## Loading required libraries

Required libraries are loaded as follows. We also set a seed for reproducibility.

```{r message=FALSE}
library(lattice)
library(ggplot2)
library(dplyr)
library(readr)
library(caret)
library(kernlab)
library(rattle)
set.seed(1317)
```

## Loading data

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r message=FALSE}
url1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
file1 <- "pml-training.csv"
file2 <- "pml-testing.csv"

if (!file.exists(file1)) {
        download.file(url1, file1)
}

if (!file.exists(file2)) {
        download.file(url2, file2)
}

training <- read_csv(file1, na=c("#DIV/0!","NA",""),show_col_types = FALSE) %>% 
        mutate(user_name = factor(user_name), 
               new_window = factor(new_window)) %>%
        rename(id = ...1)

testing <- read_csv(file2, na=c("#DIV/0!","NA",""),show_col_types = FALSE) %>% 
        mutate(user_name = factor(user_name), 
               new_window = factor(new_window)) %>%
        rename(id = ...1)

```

There are **`r nrow(training)`** observations of `r ncol(training)` variables in the training set. In the testing set, we have **`r nrow(testing)`** observations of the same variables.

## Cleaning the data

We will remove unnecessary variables, starting with mostly NA variables. Then, we remove the first seven columns of the dataset, which corresponds to metadata and therefore irrelevant to the outcome. 

```{r}
training <- training[, colMeans(is.na(training)) < 0.9] 
training <- training[, -c(1:7)]
```

After removing unnecessary variables, the number of variables has been reduced to **`r ncol(training)`**. Now, we partition the training set into a training set and a validation set. We will keep the original testing set for the project final quiz.

```{r}
forTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
t_sub <- training[forTrain,]
v_sub <- training[-forTrain,]
```

## Working with models

We will create and test three models: Decision Tree, Random Forests and Gradient Boosted Trees. We will set up control for training to use 3-fold cross validation.

```{r}
control <- trainControl(method="cv", number=3, verboseIter=F)
```

### Decision Tree

We create the tree:

```{r cache=TRUE}
trees <- train(classe~., data=t_sub, method="rpart",
               trControl=control, tuneLength = 5)

```
Then we make the predictions:

```{r}
pred_trees <- predict(trees, v_sub)
conmtx_trees <- confusionMatrix(pred_trees, factor(v_sub$classe))
conmtx_trees
```

### Random Forest

We create the model:

```{r  cache=TRUE}
rf <- train(classe~., data=t_sub, method="rf", 
                trControl = control, tuneLength = 5)
```

We make the predictions:

```{r}
pred_rf <- predict(rf, v_sub)
conmtx_rf <- confusionMatrix(pred_rf, factor(v_sub$classe))
conmtx_rf
```

### Gradient Boosted Trees

We create the model:

```{r  cache=TRUE}
gd_trees <- train(classe~., data=t_sub, method="gbm", trControl = control, tuneLength = 5, verbose = FALSE)
```

We make the predictions:

```{r cache=TRUE}
pred_gd_trees <- predict(gd_trees, v_sub)
conmtx_gd <- confusionMatrix(pred_gd_trees, factor(v_sub$classe))
conmtx_gd
```
The best model for this project is the **random forest** model with **0.9973** accuracy. Therefore, we chose this model to make the predictions on the test set. 

## Predictions on test set (quiz)

```{r}
predictions <- predict(rf, testing)
print(predictions)
```


## Appendix

### Figure 1: Decision Tree

```{r}
fancyRpartPlot(trees$finalModel)
```

### Figure 2: Random forest

```{r}
plot(rf)
```

### Figure 3: Gradient boosted trees

```{r}
plot(gd_trees)
```

